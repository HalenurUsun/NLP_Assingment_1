{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection System - Library Imports and Setup\n",
    "\n",
    "This notebook implements a spam detection system using both traditional machine learning and deep learning approaches.\n",
    "\n",
    "## Library Imports\n",
    "- **Standard Libraries**: pandas, numpy for data manipulation\n",
    "- **Machine Learning**: scikit-learn for traditional ML models and preprocessing\n",
    "- **Deep Learning**: TensorFlow/Keras for neural network implementation\n",
    "- **NLP**: NLTK for text processing\n",
    "- **Visualization**: matplotlib, seaborn for plotting results\n",
    "- **Model Persistence**: joblib for saving trained models\n",
    "\n",
    "The code also initializes essential NLTK components and sets the plotting style for consistent visualizations throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model saving\n",
    "import joblib\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Classes\n",
    "\n",
    "## EmailAnalyzer Class\n",
    "This class handles the analysis and generation of synthetic email features:\n",
    "- Maintains lists of legitimate and spam domains\n",
    "- Generates synthetic email addresses based on spam/non-spam labels\n",
    "- Scores email domains for spam likelihood on a 0-1 scale\n",
    "\n",
    "## TextProcessor Class\n",
    "Handles text preprocessing and feature extraction:\n",
    "- Text cleaning and normalization using NLTK\n",
    "- Stop word removal and lemmatization\n",
    "- Feature extraction including text length, word count, and word statistics\n",
    "- TF-IDF transformation for text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailAnalyzer:\n",
    "    \"\"\"Analyzes email domains for spam detection\"\"\"\n",
    "    def __init__(self):\n",
    "        # Define known legitimate domains\n",
    "        self.legitimate_domains = [\n",
    "            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', \n",
    "            'company.com', 'business.org', 'university.edu', 'corporate.net'\n",
    "        ]\n",
    "        # Define known spam domains\n",
    "        self.spam_domains = [\n",
    "            'temp-mail.org', 'fakemail.com', 'spam-domain.com', \n",
    "            'disposable.com', 'suspicious.net', 'temp-mail.net'\n",
    "        ]\n",
    "    \n",
    "    def generate_synthetic_emails(self, df):\n",
    "        \"\"\"Generate synthetic email addresses based on spam/non-spam label\"\"\"\n",
    "        synthetic_emails = []\n",
    "        \n",
    "        for idx, label in enumerate(df.iloc[:, -1]):\n",
    "            # Generate different usernames and domains based on spam status\n",
    "            if label == 0:  # Non-spam\n",
    "                domain = np.random.choice(self.legitimate_domains)\n",
    "                username = f\"user{idx}\"\n",
    "            else:  # Spam\n",
    "                domain = np.random.choice(self.spam_domains)\n",
    "                username = f\"temp{idx}\"\n",
    "            \n",
    "            email = f\"{username}@{domain}\"\n",
    "            synthetic_emails.append(email)\n",
    "        \n",
    "        return synthetic_emails\n",
    "    \n",
    "    def analyze_domain(self, email):\n",
    "        \"\"\"Score email domain for spam likelihood (0-1 scale)\"\"\"\n",
    "        try:\n",
    "            domain = email.split('@')[1].lower()\n",
    "            \n",
    "            # Direct matches\n",
    "            if domain in self.spam_domains:\n",
    "                return 1.0\n",
    "            if domain in self.legitimate_domains:\n",
    "                return 0.0\n",
    "            \n",
    "            # Check suspicious patterns\n",
    "            suspicious_patterns = [\n",
    "                r'temp.*mail', r'disposable', r'fake', r'suspicious'\n",
    "            ]\n",
    "            \n",
    "            for pattern in suspicious_patterns:\n",
    "                if re.search(pattern, domain):\n",
    "                    return 0.8\n",
    "            \n",
    "            return 0.5  # Unknown domain\n",
    "            \n",
    "        except:\n",
    "            return 0.5  # Invalid email format\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Processes text content for feature extraction\"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = stop_words\n",
    "        self.tfidf = TfidfTransformer()\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Clean and normalize text using NLTK\"\"\"\n",
    "        # Convert to lowercase string\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token not in self.stop_words and token.isalnum()\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def extract_text_features(self, text):\n",
    "        \"\"\"Extract numerical features from text\"\"\"\n",
    "        text = str(text)\n",
    "        words = text.split()\n",
    "        \n",
    "        return {\n",
    "            'text_length': len(text),\n",
    "            'word_count': len(words),\n",
    "            'avg_word_length': np.mean([len(w) for w in words]) if words else 0,\n",
    "            'unique_word_ratio': len(set(words)) / len(words) if words else 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation Functions\n",
    "\n",
    "## Key Functions:\n",
    "1. `load_and_enhance_data(filepath)`:\n",
    "   - Loads the dataset\n",
    "   - Enhances it with synthetic email features\n",
    "   - Processes text features\n",
    "   - Combines all features into a final feature set\n",
    "\n",
    "2. `prepare_data(X, y)`:\n",
    "   - Handles data splitting and scaling\n",
    "   - Implements different scaling strategies for different models:\n",
    "     - MinMaxScaler for Naive Bayes\n",
    "     - StandardScaler for Neural Networks\n",
    "   - Calculates class weights for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_enhance_data(filepath):\n",
    "    \"\"\"Load dataset and enhance it with synthetic features\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {df.shape[1]}\")\n",
    "    \n",
    "    # Generate synthetic emails and analyze domains\n",
    "    email_analyzer = EmailAnalyzer()\n",
    "    synthetic_emails = email_analyzer.generate_synthetic_emails(df)\n",
    "    email_scores = [email_analyzer.analyze_domain(email) for email in synthetic_emails]\n",
    "    \n",
    "    # Create email features DataFrame\n",
    "    features_df = pd.DataFrame({\n",
    "        'domain_score': email_scores\n",
    "    })\n",
    "    \n",
    "    # Process text features\n",
    "    text_processor = TextProcessor()\n",
    "    text_features = []\n",
    "    \n",
    "    print(\"\\nProcessing text features...\")\n",
    "    for _, row in tqdm(df.iloc[:, 1:-1].iterrows(), total=len(df)):\n",
    "        # Combine all word frequencies into text\n",
    "        text = ' '.join(f\"{k}:{v}\" for k, v in row.items())\n",
    "        features = text_processor.extract_text_features(text)\n",
    "        text_features.append(features)\n",
    "    \n",
    "    # Convert text features to DataFrame\n",
    "    text_features_df = pd.DataFrame(text_features)\n",
    "    \n",
    "    # Combine all features\n",
    "    X = pd.concat([\n",
    "        df.iloc[:, 1:-1],  # Original features\n",
    "        features_df,       # Email features\n",
    "        text_features_df   # Text features\n",
    "    ], axis=1)\n",
    "    \n",
    "    y = df.iloc[:, -1]    # Labels\n",
    "    \n",
    "    print(\"\\nFinal feature set:\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Added features: {list(features_df.columns) + list(text_features_df.columns)}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    \"\"\"Prepare and scale data for different models\"\"\"\n",
    "    # Calculate class weights for imbalanced data\n",
    "    n_samples = len(y)\n",
    "    n_classes = len(np.unique(y))\n",
    "    class_weights = dict(enumerate(n_samples / (n_classes * np.bincount(y))))\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Store input dimension\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Scale data differently for different models\n",
    "    # For Naive Bayes: Use MinMaxScaler (ensures non-negative values)\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_train_nb = minmax_scaler.fit_transform(X_train)\n",
    "    X_test_nb = minmax_scaler.transform(X_test)\n",
    "    \n",
    "    # For Neural Networks: Use StandardScaler\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train_nn = standard_scaler.fit_transform(X_train)\n",
    "    X_test_nn = standard_scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\nData Split Information:\")\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    print(f\"Input dimension: {input_dim}\")\n",
    "    \n",
    "    return {\n",
    "        'nb': (X_train_nb, X_test_nb, y_train, y_test),\n",
    "        'nn': (X_train_nn, X_test_nn, y_train, y_test),\n",
    "        'input_dim': input_dim\n",
    "    }, class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Framework\n",
    "\n",
    "## ModelEvaluator Class\n",
    "Comprehensive evaluation system that:\n",
    "- Generates confusion matrices\n",
    "- Plots ROC curves and calculates AUC scores\n",
    "- Creates precision-recall curves\n",
    "- Saves evaluation metrics and visualizations\n",
    "\n",
    "## NaiveBayesSpam Class\n",
    "Implementation of the Naive Bayes classifier:\n",
    "- Uses ComplementNB for better handling of imbalanced data\n",
    "- Includes cross-validation\n",
    "- Saves model and evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Handles model evaluation and visualization\"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def evaluate_and_plot(self, y_true, y_pred, y_pred_proba=None):\n",
    "        \"\"\"Generate comprehensive evaluation metrics and plots\"\"\"\n",
    "        # Calculate metrics\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        # Save plots to results folder\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Non-Spam', 'Spam'],\n",
    "                   yticklabels=['Non-Spam', 'Spam'])\n",
    "        plt.title(f'Confusion Matrix - {self.model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(f'results/confusion_matrix_{self.model_name.lower().replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        if y_pred_proba is not None:\n",
    "            # ROC Curve\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "            auc = roc_auc_score(y_true, y_pred_proba)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {self.model_name}')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'results/roc_curve_{self.model_name.lower().replace(\" \", \"_\")}.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # Precision-Recall Curve\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.plot(recall, precision)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title(f'Precision-Recall Curve - {self.model_name}')\n",
    "            plt.savefig(f'results/pr_curve_{self.model_name.lower().replace(\" \", \"_\")}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Save classification report\n",
    "        with open(f'results/report_{self.model_name.lower().replace(\" \", \"_\")}.txt', 'w') as f:\n",
    "            f.write(f\"Classification Report - {self.model_name}:\\n\")\n",
    "            f.write(classification_report(y_true, y_pred))\n",
    "        \n",
    "        return {\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'f1_score': f1,\n",
    "            'auc_score': auc if y_pred_proba is not None else None\n",
    "        }\n",
    "\n",
    "class NaiveBayesSpam:\n",
    "    \"\"\"Naive Bayes model for spam detection\"\"\"\n",
    "    def __init__(self, class_weights=None):\n",
    "        self.model = ComplementNB(alpha=0.5)\n",
    "        self.evaluator = ModelEvaluator('Naive Bayes')\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5, scoring='f1')\n",
    "        \n",
    "        # Save cross-validation results\n",
    "        with open('results/nb_cv_results.txt', 'w') as f:\n",
    "            f.write(\"Cross-validation Results:\\n\")\n",
    "            f.write(f\"F1 scores: {cv_scores}\\n\")\n",
    "            f.write(f\"Average F1: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model\n",
    "        joblib.dump(self.model, 'results/naive_bayes_model.joblib')\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        return self.evaluator.evaluate_and_plot(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer MLP Implementation\n",
    "\n",
    "## TwoLayerMLP Class\n",
    "Neural network architecture with:\n",
    "- Two hidden layers (128 and 64 units)\n",
    "- Batch normalization and dropout for regularization\n",
    "- Binary classification output\n",
    "- Comprehensive training pipeline including:\n",
    "  - Early stopping\n",
    "  - Learning rate reduction\n",
    "  - Model checkpointing\n",
    "  - Training history visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP:\n",
    "    \"\"\"Two-layer MLP for spam detection\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        # Initialize sequential model with exactly two layers as per requirements\n",
    "        self.model = Sequential([\n",
    "            # First layer\n",
    "            Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Second layer\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            # Output layer (not counted as a layer since it's just the prediction layer)\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        self.evaluator = ModelEvaluator('Two-Layer MLP')\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
    "        # Convert to numpy arrays if needed\n",
    "        if hasattr(X_train, 'to_numpy'):\n",
    "            X_train = X_train.to_numpy()\n",
    "        if hasattr(X_test, 'to_numpy'):\n",
    "            X_test = X_test.to_numpy()\n",
    "        if hasattr(y_train, 'to_numpy'):\n",
    "            y_train = y_train.to_numpy()\n",
    "        if hasattr(y_test, 'to_numpy'):\n",
    "            y_test = y_test.to_numpy()\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'AUC']\n",
    "        )\n",
    "        \n",
    "        # Training callbacks\n",
    "        callbacks = [\n",
    "            # Early stopping to prevent overfitting\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=5, \n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            # Reduce learning rate when training plateaus\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            # Save best model during training\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'results/best_mlp_model.h5',\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Create results directory if it doesn't exist\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save training history plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Two-Layer MLP Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('results/mlp_training_history.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save training metrics\n",
    "        metrics_df = pd.DataFrame(history.history)\n",
    "        metrics_df.to_csv('results/mlp_training_metrics.csv')\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = (self.model.predict(X_test) > 0.5).astype(int)\n",
    "        y_pred_proba = self.model.predict(X_test).ravel()\n",
    "        \n",
    "        # Save model architecture\n",
    "        with open('results/mlp_architecture.txt', 'w') as f:\n",
    "            self.model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        # Evaluate model and save results\n",
    "        evaluation_results = self.evaluator.evaluate_and_plot(y_test, y_pred, y_pred_proba)\n",
    "        \n",
    "        # Save model performance metrics\n",
    "        with open('results/mlp_performance.txt', 'w') as f:\n",
    "            f.write(\"Two-Layer MLP Performance Metrics\\n\")\n",
    "            f.write(\"================================\\n\")\n",
    "            f.write(f\"F1 Score: {evaluation_results['f1_score']:.4f}\\n\")\n",
    "            f.write(f\"AUC Score: {evaluation_results['auc_score']:.4f}\\n\")\n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            f.write(str(evaluation_results['confusion_matrix']))\n",
    "        \n",
    "        return evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution Pipeline\n",
    "\n",
    "## Key Components:\n",
    "1. `save_final_report()`: Generates comprehensive comparison report\n",
    "2. `main()`: Orchestrates the entire training and evaluation process:\n",
    "   - Data loading and preparation\n",
    "   - Model training and evaluation\n",
    "   - Results compilation and saving\n",
    "   \n",
    "The pipeline creates a structured output in the 'results' directory containing:\n",
    "- Model performance metrics\n",
    "- Visualization plots\n",
    "- Trained models\n",
    "- Detailed comparison reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preparing data...\n",
      "Loading dataset...\n",
      "\n",
      "Dataset Statistics:\n",
      "Total rows: 5172\n",
      "Total columns: 3002\n",
      "\n",
      "Processing text features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5172/5172 [00:10<00:00, 471.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final feature set:\n",
      "Number of features: 3005\n",
      "Added features: ['domain_score', 'text_length', 'word_count', 'avg_word_length', 'unique_word_ratio']\n",
      "\n",
      "Data Split Information:\n",
      "Training set size: 4137\n",
      "Test set size: 1035\n",
      "Class weights: {0: 0.704248366013072, 1: 1.724}\n",
      "Input dimension: 3005\n",
      "\n",
      "Step 2: Training Naive Bayes Model...\n",
      "\n",
      "Step 3: Training Two-Layer MLP Model...\n",
      "Epoch 1/50\n",
      "104/104 [==============================] - 3s 14ms/step - loss: 0.3154 - accuracy: 0.8673 - auc: 0.9552 - val_loss: 0.1177 - val_accuracy: 0.9601 - val_auc: 0.9929 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0729 - accuracy: 0.9825 - auc: 0.9987 - val_loss: 0.0789 - val_accuracy: 0.9746 - val_auc: 0.9994 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0392 - accuracy: 0.9897 - auc: 0.9995 - val_loss: 0.0326 - val_accuracy: 0.9928 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0223 - accuracy: 0.9943 - auc: 0.9999 - val_loss: 0.0308 - val_accuracy: 0.9915 - val_auc: 0.9998 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0186 - accuracy: 0.9961 - auc: 0.9997 - val_loss: 0.0303 - val_accuracy: 0.9940 - val_auc: 0.9990 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0110 - accuracy: 0.9976 - auc: 1.0000 - val_loss: 0.0262 - val_accuracy: 0.9952 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0113 - accuracy: 0.9967 - auc: 1.0000 - val_loss: 0.0170 - val_accuracy: 0.9940 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0153 - accuracy: 0.9967 - auc: 0.9999 - val_loss: 0.0601 - val_accuracy: 0.9783 - val_auc: 0.9983 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0125 - accuracy: 0.9967 - auc: 0.9999 - val_loss: 0.0233 - val_accuracy: 0.9915 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0085 - accuracy: 0.9967 - auc: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9928 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0065 - accuracy: 0.9988 - auc: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9903 - val_auc: 0.9996 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 0.0045 - accuracy: 0.9985 - auc: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9940 - val_auc: 0.9998 - lr: 5.0000e-04\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "\n",
      "Step 4: Generating final report...\n",
      "\n",
      "Training complete! Results saved in 'results' directory.\n"
     ]
    }
   ],
   "source": [
    "def save_final_report(results):\n",
    "    \"\"\"Generate and save final comparison report\"\"\"\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    with open('results/final_comparison_report.txt', 'w') as f:\n",
    "        f.write(\"SPAM DETECTION SYSTEM - FINAL RESULTS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Compare model performances\n",
    "        f.write(\"MODEL COMPARISON\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        for model_name, metrics in results.items():\n",
    "            f.write(f\"\\n{model_name} Results:\\n\")\n",
    "            f.write(f\"F1 Score: {metrics['f1_score']:.4f}\\n\")\n",
    "            if metrics['auc_score'] is not None:\n",
    "                f.write(f\"AUC Score: {metrics['auc_score']:.4f}\\n\")\n",
    "            \n",
    "            # Add confusion matrix\n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            f.write(str(metrics['confusion_matrix']))\n",
    "            f.write(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        # Add timestamp and summary\n",
    "        f.write(f\"\\nReport generated at: {pd.Timestamp.now()}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Create results directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    X, y = load_and_enhance_data('data/emails.csv')\n",
    "    scaled_data, class_weights = prepare_data(X, y)\n",
    "    \n",
    "    # Store results for each model\n",
    "    results = {}\n",
    "    \n",
    "    # Train and evaluate Naive Bayes\n",
    "    print(\"\\nStep 2: Training Naive Bayes Model...\")\n",
    "    nb_model = NaiveBayesSpam(class_weights=class_weights)\n",
    "    X_train_nb, X_test_nb, y_train_nb, y_test_nb = scaled_data['nb']\n",
    "    results['Naive Bayes'] = nb_model.train_and_evaluate(\n",
    "        X_train_nb, X_test_nb, y_train_nb, y_test_nb\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate Two-Layer MLP\n",
    "    print(\"\\nStep 3: Training Two-Layer MLP Model...\")\n",
    "    input_dim = scaled_data['input_dim']\n",
    "    mlp_model = TwoLayerMLP(input_dim)\n",
    "    X_train_nn, X_test_nn, y_train_nn, y_test_nn = scaled_data['nn']\n",
    "    results['Two-Layer MLP'] = mlp_model.train_and_evaluate(\n",
    "        X_train_nn, X_test_nn, y_train_nn, y_test_nn\n",
    "    )\n",
    "    \n",
    "    # Generate final report\n",
    "    print(\"\\nStep 4: Generating final report...\")\n",
    "    save_final_report(results)\n",
    "    \n",
    "    print(\"\\nTraining complete! Results saved in 'results' directory.\")\n",
    "    return results\n",
    "\n",
    "# Execute main function if running as script\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
